
Overview of LiveAvatar
LiveAvatars are programmable interfaces that give a human touch to your AI Agents!

Introduction
A LiveAvatar is an AI-powered digital human that can interact with users in real time with both audio and video. Developers can integrate LiveAvatars into their apps or websites to create natural, conversational experiences such as:

Real-time product demos or virtual sales assistants
AI-powered support or training agents
Interactive hosts, tutors, or characters
At the heart of the system is the LiveAvatar Session — a live, persistent connection that allows users to speak or chat with an avatar. The session handles:

Streaming in user input (voice or text)
Feeding it into a language model (LLM) and generating a response
Rendering the response as with synchronized speech and video
This means every conversation feels dynamic, personal, and fully real-time.

Getting Started with LiveAvatar
We have two main ways to build with LiveAvatar: FULL mode and Custom mode.

FULL Mode: we host and manage the various services need to make the avatar conversation flow, including the ASR, LLM, TTS and most importantly the avatar streaming in a WebRTC room. You get to control the finer details such as avatar, voice and context used, while trusting our provided default configurations.
CUSTOM MODE: we mange the the solely the avatar streaming framework. You'll get to control the WebRTC integration and bring your own STT, LLM, and TTS frameworks. You'll need to manage the orchestration and infrastructure associated.
LiveAvatar Session
The Session is a core piece of the LiveAvatar API — it represents a single, continuous stream of interactions between a user and an avatar. Developers can observe and control interactions through a set of events and callbacks.

Under the hood, the Session manages connectivity, user input, avatar output, and conversational state. Throughout the session’s lifetime, the API emits events that let developer applications stay in sync with what’s happening — whether that’s a user speaking, the avatar responding, or the connection closing.

Quickstart
Set up your first conversation with a LiveAvatar.

We recommend developing with both a backend server and a managed frontend client. However, while you're prototyping and testing, this is a quick guide to help you have your first conversation with a LiveAvatar.

1. Create a Session Token
Before starting a session, generate a session token on your backend server. This token defines the configuration for the LiveAvatar session and also serves as a short-lived access token, allowing your frontend client to manage the session once it starts.

You can find your API token on the LiveAvatar settings page.

See the example curl command below to generate a session token for FULL mode.

cURL

curl --request POST \
     --url https://api.liveavatar.com/v1/sessions/token \
     --header 'X-API-KEY: <API Token>' \
     --header 'accept: application/json' \
     --header 'content-type: application/json'
     --data '{
        "mode": "FULL",
        "avatar_id": <avatar_id>,
        "avatar_persona": {
          "voice_id": <voice_id>,
          "context_id": <context_id>,
          "language": "en"
         }
      }'
When successful, you'll receive both a session_id and session_token.


{
  "session_id": <new_session_id>,
  "session_token": <your_session_token>,
}
2. Start the Session
Once you have access to a session token, you're ready to start the session. This can be done via a quick call via the newly generated session_token.

cURL

curl --request POST \
     --url https://api.liveavatar.com/v1/sessions/start \
     --header 'accept: application/json' \
     --header 'authorization: Bearer <session_token>'
With this, we return a LiveKit room url along with the room token. We stream all the relevant information about the room user.

3. Join the LiveKit room
For this quick start, try directly joining the LiveKit room on your web-browser and chatting with the avatar. Simply navigate to the LiveKit URL to experience the it for yourself.


https://meet.livekit.io/custom?liveKitUrl=[livekit_url]&token=[livekit_client_token]
Once you join the room, you'll now be chatting with a LiveAvatar!

4. Building your own experience
Let's now get you up and ready to build your own customized experiences. We recommend starting up your client facing services with the our existing SDKs and using our demos to help get additional inspiration. In addition, if while you're building and prototyping, we recommend using sandbox mode to help conserve credits.


If you're building with npm, we recommend using our own npm package to help to abstract away the session management of LiveKit.

Examples - https://github.com/heygen-com/liveavatar-web-sdk
NPM - https://www.npmjs.com/package/@heygen/liveavatar-web-sdk?activeTab=versions
You can also check out our API documentation to see all available functionality.

Developing in Sandbox Mode
Developing on top of LiveAvatar sessions can consume a lot of credits.

That's why we've decided to build Sandbox Mode. Our intent is to unblock developers build and iterate on their API integrations, without burning through credits better suited for production.

What is Sandbox Mode
Sandbox Mode is a special execution mode for LiveAvatar sessions designed for development and testing. It allows you to integrate, experiment, and validate your LiveAvatar workflows without consuming credits.

Verifying your API integration
Testing session lifecycle
Iterating on frontend or backend
Debugging any existing audio/video pipelines
While Sandbox Mode mirrors production behavior, it intentionally applies strict limits to prevent excessive resource usage.

Sandbox Mode Behaviors
In sandbox mode, the following behaviors apply:

Avatar Restrictions - developers will only be able to use a select subset of production avatars.
Wayne - dd73ea75-1218-4ef3-92ce-606d5f7fbc0a (id)
Session Duration Limits - sandbox mode sessions will terminate in around 1 minute. The session will automatically close and be treated as a maximum duration reached.
No credit usage - sandbox mode does not require any credits to start and will not consume any credits.
These constraints ensure Sandbox Mode remains a safe and cost-free environment for development while still providing realistic behavior.

Enabling Sandbox Mode
To enable Sandbox Mode, include the is_sandbox flag when creating a LiveAvatar token Create Session Token.



FULL Mode Lifecycle
1. Starting a Session
Starting a session has two pieces:

Generating a session token.
Starting the session with the session token.
To start in FULL mode, ensure that the session token is set to FULL mode. An example flow of how FULL mode is started will look something as follows:


When users first visit your site, create a session token that can be used to manage the session lifecycle and pass it back to your frontend application. The returned session token can then be used by your frontend application to directly manage the session, including starting/stopping the session.
We recommend then building the rest of your application out solely on your frontend.
Start the session. Depending on your use case, you may want to immediately start the session after the token is retrieved, in which your backend application can directly call session.start. Or start the session after a user clicks a "start session" button, in which your frontend can make that call.
In our example, we assumed that your frontend application enabled the start call.
Connect to the the LiveAvatar managed room
After starting the session, we will provide the necessary information to establish a WebRTC connection between the end user's device and our LiveAvatar room.
Simply connect the user's device to the Room and the session will be live.
2. Managing the Session
After connecting to the room, managing the session will be done purely via an event driven architecture. Sending command events will enable you to control certain actions. On LiveAvatar side, we also emit server events to enable you to track the whole state of the conversation.

Using client-side and server side events, you will have control over:

when the user/avatar is speaking.
transcript data on what the user/avatar is saying
control over avatar behaviors such as:
interrupting
changing between idle and listening state
direct control on what you want the avatar to say / respond to, in addition to processing user audio input.
See FULL mode's event list to see the up to date supported events.

Event data is transmitted via LiveKit room data channels.

Recommended Architecture

In FULL mode, we recommend making your frontend the primary controller for emitting events from the LiveAvatar room. This approach minimizes latency between user interactions and LiveAvatar responses.

Any additional event data can be forwarded from the frontend to your backend for downstream processing, logging, and analytics.

3. Ending the Session
When the session is closed, we will do the following.

Close the LiveKit room
Our Avatar will leave the room
We also remove the user from the room if still present.
While there are no resources that we expect the developers to manage, we also recommend cleaning up any resources that is used externally.

Configuration Options
As of right now, we offer the following ways to personalize your LiveAvatar experience specific to FULL mode.

Avatar (Visual Layer) — Defines what the avatar looks like. Choose from a wide selection of avatars, each with unique styles, appearances, and expressions.
Voice (Audio Layer) — Defines what the avatar sounds like. Choose a voices that fit your needs — from calm and professional to energetic or youthful.
Context (Cognitive Layer) — Defines how the avatar thinks and responds. Control the personality traits, background knowledge, and behavior, which guide how the LLM generates responses.
Interactivity Type (Conversational Layer) — Controls how/when you want the avatar to respond.
Each of these layers can be configured when creating a session, giving developers precise control over the avatar’s look, sound, and intelligence.

Avatars
An avatar determines the visual appearance of your AI agent. Developers can select from a catalog of avatars — ranging from realistic human presenters to stylized or expressive characters.

Each avatar can be uniquely identified by its avatar_id.

You can preview all available avatars here.

Avatar Personas
In addition to controlling the look of the Avatar, you can control additional features of the Avatar including it's voice and the context (think of it as the Avatar's personality).

Voices
A voice defines the sound and personality of your avatar’s speech. Voices can have many variabilities including gender, age, tones, and accents. Some voices are also better at some languages compared to others.

In addition, you can control features such as the speed, style and stability of the voice by tuning the voice_settings field. These features allow you to flexibly control the generated audio.

You can preview all available voices here.

Context
The context allows you to shape how your avatar will think and respond. This layer defines the information available to the avatar, the constraints replied to it, and any quirky personalities you want your avatar to have.

Each context includes an opening text, that will be said at the start of each LiveAvatar session, and instructions to direct the responses you want your avatar to generate. If no opening text is provided in the context, then we will not say anything to begin with.

What happens if no context is passed in
Contexts can be passed in when first creating a session token. If no context is supplied, the avatar will operate in a more restricted manner.

The avatar will not generate any responses to user conversation input.
We will still emit user transcript events, which transcribe what the user says.
Avatars can still repeat phrases that you want them to say.
You can manage all your contexts here.

Interactivity Type
As of now, there are two interactivity types we support:

Conversational - we manage changes in conversation and control when the avatar start/stop responding. This is in response to pauses in users speech or when the user tries to interrupt the avatar. This is the default interactivity type.
Push-to-Talk - you have full control of when the user's input is registered by signaling the exact moments user start/stop talking. We disable our VAD/ASR controls and give you precise control of when you want the avatar to respond.
Shared Configuration Options
These configurations are available in both FULL mode and LITE mode.

Video Settings
quality : control the output video resolution. By default, we recommend high. Increasing resolution negatively impact streaming latency and throughput.
very_high: 1080p
high: 720p
medium: 480p
low: 360p
encoding: controls the video encoding. Right now we support, VP8 and H264 as options.
Wrapping Up
Combining the everything together, you can customize your LiveAvatar to make the avatar look, sound, and respond consistently throughout the conversation, exactly how you want it to.


Push-To-Talk
Supporting explicit detection of user conversation

Overview
In real conversations, people don’t always speak in clean, uninterrupted sentences. Users may pause to gather their thoughts, correct themselves mid-sentence, or briefly stop talking without actually being done. In these cases, relying purely on automatic speech or silence detection can lead to premature interruptions or incomplete responses.

To address this, we provide Push-to-Talk (PTT) — a mechanism that gives developers explicit control over when user speech starts and ends. Instead of the system guessing when the user is finished speaking, your application tells us exactly when to listen and when to process.

Why Push-to-Talk?
Push-to-Talk is ideal when:

Your users have non-linear or thoughtful conversational patterns
Short pauses should not be interpreted as the end of speech
You want deterministic control over speech boundaries
You’re implementing custom UI/UX (e.g. press-and-hold, toggle buttons, keyboard shortcuts)
By explicitly defining the speaking window, you ensure that only the intended audio is processed and responded to.

High-Level Flow
The Push-to-Talk interaction follows a simple, explicit lifecycle:

Start Push-to-Talk
Your application signals that the user is beginning to speak.
The system starts buffering and recording user audio.
Send the event,
User Speaks
The user can talk freely, including pauses or corrections.
No response is generated during this phase.
End Push-to-Talk
Your application signals that the user has finished speaking.
Audio capture stops.
Processing & Response
The avatar processes only the audio captured between the start and end signals.
The avatar generates and delivers its response.
Implementation Notes
When starting the session, explicitly set interactivity_type to be PUSH_TO_TALK
Push-to-Talk command events should be sent by application.
The start event is user.start_push_to_talk
The end event is user.stop_push_to_talk
The system will not attempt to infer speech completion while PTT is active.
We emit the corresponding server events.
user.push_to_talk_started
user.push_to_talk_start_failed
user.push_to_talk_stopped
user.push_to_talk_stop_failed
Any audio outside of a PTT window is ignored.
You are recommended to map PTT to UI gestures such as:
Press-and-hold buttons
Toggle switches
Keyboard shortcuts
Touch or controller input
Summary
Push-to-Talk shifts control from automatic speech detection to your application. By explicitly defining when speech starts and ends, you gain predictable, interruption-free conversations that better match real human speaking patterns.

Use PTT when conversational timing matters — and let your users speak at their own pace.

In FULL Mode, we process events that are sent via the LiveKit Protocol. Simply send these events into the LiveKit room session and we will process all of them. All FULL mode events will be structured in the following JSON format.


{
  "event_type": str, // name of the event
  "session_id": str, // session id
  ... // event data passed passed in. 
}
To differentiate between command and server events, we track different LiveKit room topics. You will need to publish/subscribe to specific topics in-order to track this data.

Command Events
For command events, these events need to be published to this topic: agent-control.

Below is the list of all events available that allow you to better control the state of the avatar.

Name / event_type

Additional Event Data

Description

avatar.interrupt

N/A

Interrupts and stops any scheduled commands. This will disrupt the entire sequence of calls passed in.

avatar.speak_text

{"text": string}

Instructs the avatar to say the spoken text

avatar.speak_response

{"text": string}

Instruct the avatar to generate an LLM response to the input text

avatar.start_listening

N/A

Instructs the avatar to switch to a listening state. Most of the time, avatars sit in an idle state when they are not speaking.

avatar.stop_listening

N/A

Instructs the avatar to leave the listening state and return to the idle state.

user.start_push_to_talk

N/A

(Push-To-Talk Interactivity Only)
Signal to start taking in user audio

user.stop_push_to_talk

N/A

(Push-To-Talk Interactivity Only)
Signal to stop taking in user audio

Server Events
For server events, these events will be emitted to the topic, agent-response

Throughout the Session, we will emit these events. Developers will be able to listen in to whichever events they deem necessary to build additional experiences on the user interface.

Name / event_type

Additional Event Data

Description

user.speak_started

N/A

The user in the LiveKit room has started sending us audio data.

user.speak_ended

N/A

The user in the LiveKit room is no longer sending us audio data.

Always follows a user.speak_started event

avatar.speak_started

N/A

The avatar in the LiveKit room started sending audio output

avatar.speak_ended

N/A

The avatar in the LiveKit room finished sending audio output.

Always follows a avatar.speak_started event.

user.transcription

{"text": string}

We've finished processing the input audio. The transcribed text is included in the event data under text.

avatar.transcription

{"text": string}

The avatar's text response.

When calling avatar.speak_text, we will immediately emit this event and avatar.transcription_ended with the input text.

When calling avatar.speak_response, we emit this event after LLM generation.

user.push_to_talk_started

N/A

(Push-To-Talk Interactivity Only)
Success on processing client push to talk start event

user.push_to_talk_start_failed

N/A

(Push-To-Talk Interactivity Only)
Fail to process client push to talk event

user.push_to_talk_started

N/A

(Push-To-Talk Interactivity Only)
Success on processing client push to talk stop event

user.push_to_talk_stop_failed

N/A

(Push-To-Talk Interactivity Only)
Fail to process client push to talk stop event

session.stopped

{"end_reason": string}

When the session ends, we emit this event along with the reason the session was stopped.


LITE Mode Life Cycle
1. Starting a Session
Starting a session has two pieces:

Generating a session token.
Starting the session with the session token.
To start in CUSTOM mode, ensure that the session token is set to CUSTOM mode. See here for more details on configuring LiveAvatar - https://docs.liveavatar.com/docs/configuring-custom-mode

An example flow looks something as follows:


When the user first visits the site, create a session token that can be used to call various API requests. We recommend having this be managed by your backend instance.
Start the session. Using the session token you will be able to start/end the session.
In our example, we recommend having your backend manage both the token generation and session start call.
We send the relevant session information over.
Once the session is started, we send our avatar stream into the specified WebRTC room. It's up to the developer to then:
Connect their agent/API into this room.
Connect the user into this room.
2. Managing the Session
In CUSTOM mode, we provide a websocket connection that will enable developers to send any avatar specific events over. This includes:

Commanding the avatar to say something.
Interrupting the avatar.
Changing the avatar's pose.
Recommended Architecture

The recommended flow of data is as follows:

User starts speaking and data is sent to the room
Developer's agent listens into the room and processes the user input
It then constructs the response audio that should be relayed back
Developer's agent then streams the response to our API via the websocket
LiveAvatar will then send to the room the avatar video frame data, which then gets displayed to the user.
Although we call it "agent" here, this can also be a simple backend service dedicated to processing audio from the room.

3. Ending the Session
When the session is closed, we will do the following.

Our Avatar will leave the current LiveKit room.
If we spun up a LiveKit room, we will also teardown said room.
Close the websocket connection.
We recommend cleaning up resources that currently publish/subscribe to events from the websocket. In addition, you'll need to manage cleaning up any personal LiveKit data and managing the user's room experience.


LITE Mode Configuration
Configuration Options
As of now, we offer the customization for:

Avatars - control how the avatar looks
WebRTC configurations - control where you want us to send our Avatar. We support
LiveKit
Agora
Avatars
An avatar determines the visual appearance of your AI agent. Developers can select from a catalog of avatars — ranging from realistic human presenters to stylized or expressive characters.

Each avatar can be uniquely identified by its avatar_id.

You can preview all available avatars here.

WebRTC Configuration
LiveAvatar uses LiveKit to power our WebRTC setup.

In LITE mode, when a session starts, we create a LiveKit room and emit an agent token that you can use to connect your own custom LiveKit Agent to the room. In this mode, we only provide the WebRTC room and media transport — you are responsible for configuring and running your own ASR, LLM, and TTS stack.

If you prefer full control, you can also bring your own WebRTC infrastructure. Provide the required room and connection configuration, and we will stream the Avatar video directly into your WebRTC setup. We currently support:

Agora - via agora_config
LiveKit - via livekit_config


LITE Mode Events
In LITE Mode, we will process events sent through via the websocket we expose. These events are transmitted as JSON formatted strings. The general format will be as follows:


{
  "type": str, 
  ... // event data
}
Expect to process them as JSON strings and send the event data to use as JSON strings via the websocket. If you any events you'd like us to pass in, we'd recommend giving feedback and recommendations.

Command Events
In LITE Mode, you can send us events via websocket endpoint. As of now, we primarily support transitioning the Avatar between speaking and listening, as well as interrupting any existing events that are happening.

Name / type

Event Data

Description

agent.interrupt

{"type": "agent.interrupt"}

Interrupts and stops any scheduled commands.
This will clear all scheduled speaking events.

agent.speak

{"type": "agent.speak", "audio": <string>}

Adds to the avatar buffer for what it will say. The event_id can be set to indicate grouping between sets of interactions.

Audio should be PCM 16Bit 24KHz bytes encoded as Base64

Note - we recommend sending chunked audio bytes. The recommended packet size is around 1 second of audio. We may close the connection if we detect too large of an audio string input (> 1MB).

avatar.speak_end

{ "type": "agent.speak_end", "event_id": <str> }

Signal the end of a speaking event.

agent.start_listening

{"type":"agent.start_listening", "event_id": <str>}

Transition the avatar to a listening state.

agent.stop_listening

{"type":"agent.stop_listening", "event_id": <str>}

Transition the avatar from a listening state to an idle state.

session.keep_alive

{ "type": "session.keep_alive", "event_id": "<event_id>", }

Keep the session alive longer. We close the session after a 5 minute idle time window. Periodically send this event if you want to keep the session alive longer.


Server Events
We will emit these events from the websocket that is public when calling LITE mode. This is a smaller set of events.

Name

Event Data

Description

session.state_updated

{ "type": "session.state_updated", "event_id": uuid, "state": strEnum}

We track what the current state of the session is. Prior to sending any events, we require the state to be "connected"

Possible states:
"connected", "connecting", "closed", "closing".

agent.speak_started

{ "type":"agent.speak_started", "event_id": <str>, "task": {"id": <str> } }

Avatar started speaking. The event id matches the corresponding, agent.speak command event.

agent.speak_ended

{"type":"agent.speak_started", "event_id": <str>, "task": {"id": <str> }}

Avatar started speaking. The event id matches the corresponding, agent.speak command event.

Note - should always follow a agent.speak_started event.

Integrations and Plugins
We have plugins in the work, built directly on top of LiveAvatar CUSTOM mode. This makes getting started or extending implementations easier.

WebRTC Providers

Provider

Link

LiveKit

Virtual Avatar Plugin
Pipecat / Daily

Pipecat Video
Pipecat Transport

